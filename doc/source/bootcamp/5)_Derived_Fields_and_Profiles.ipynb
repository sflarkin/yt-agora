{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Derived Fields and Profiles\n",
      "\n",
      "One of the most powerful features in yt is the ability to create derived fields that act and look exactly like fields that exist on disk.  This means that they will be generated on demand and can be used anywhere a field that exists on disk would be used.  Additionally, you can create them by just writing python functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from yt.imods import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Derived Fields\n",
      "\n",
      "This is an example of the simplest possible way to create a derived field.  All derived fields are defined by a function and some metadata; that metadata can include units, LaTeX-friendly names, conversion factors, and so on.  Fields can be defined in the way in the next cell.  What this does is create a function which accepts two arguments and then provide the units for that field.  In this case, our field is `Dinosaurs` and our units are `Trex/s`.  The function itself can access any fields that are in the simulation, and it does so by requesting data from the object called `data`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@derived_field(units = \"Trex/s\")\n",
      "def Dinosaurs(field, data):\n",
      "    return data[\"Density\"]**(2.0/3.0) * data[\"VelocityMagnitude\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One important thing to note is that derived fields must be defined *before* any datasets are loaded.  Let's load up our data and take a look at some quantities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pf = load(\"IsolatedGalaxy/galaxy0030/galaxy0030\")\n",
      "dd = pf.h.all_data()\n",
      "print dd.quantities.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['MinLocation', 'StarAngularMomentumVector', 'WeightedVariance', 'TotalMass', 'AngularMomentumVector', 'TotalQuantity', 'IsBound', 'WeightedAverageQuantity', 'CenterOfMass', 'BulkVelocity', 'ParticleSpinParameter', 'Action', 'Extrema', 'MaxLocation', 'BaryonSpinParameter']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One interesting question is, what are the minimum and maximum values of dinosaur production rates in our isolated galaxy?  We can do that by examining the `Extrema` quantity -- the exact same way that we would for Density, Temperature, and so on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dd.quantities[\"Extrema\"](\"Dinosaurs\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(2.2146366774504352e-20, 9.1573883828992124e-09)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can do the same for the average quantities as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dd.quantities[\"WeightedAverageQuantity\"](\"Dinosaurs\", weight=\"Temperature\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A Few Other Quantities\n",
      "\n",
      "We can ask other quantities of our data, as well.  For instance, this sequence of operations will find the most dense point, center a sphere on it, calculate the bulk velocity of that sphere, calculate the baryonic angular momentum vector, and then the density extrema.  All of this is done in a memory conservative way: if you have an absolutely enormous dataset, yt will split that dataset into pieces, apply intermediate reductions and then a final reduction to calculate your quantity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp = pf.h.sphere(\"max\", (10.0, 'kpc'))\n",
      "bv = sp.quantities[\"BulkVelocity\"]()\n",
      "L = sp.quantities[\"AngularMomentumVector\"]()\n",
      "(rho_min, rho_max), = sp.quantities[\"Extrema\"](\"Density\")\n",
      "print bv, L, rho_min, rho_max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Profiles\n",
      "\n",
      "yt provides the ability to bin in 1, 2 and 3 dimensions.  This means discretizing in one or more dimensions of phase space (density, temperature, etc) and then calculating either the total value of a field in each bin or the average value of a field in each bin.\n",
      "\n",
      "We do this using the objects `BinnedProfile1D`, `BinnedProfile2D`, and `BinnedProfile3D`.  The first two are the most common since they are the easiest to visualize.\n",
      "\n",
      "This first set of commands manually creates a `BinnedProfile1D` from the sphere we created earlier, binned in 32 bins according to density between `rho_min` and `rho_max`, and then takes the Density-weighted average of the fields `Temperature` and (previously-defined) `Dinosaurs`.  We then plot it in a loglog plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prof = BinnedProfile1D(sp, 32, \"Density\", rho_min, rho_max)\n",
      "prof.add_fields([\"Temperature\", \"Dinosaurs\"], weight=\"Density\")\n",
      "pylab.loglog(prof[\"Density\"], prof[\"Temperature\"], \"-x\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we plot the `Dinosaurs` field."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab.loglog(prof[\"Density\"], prof[\"Dinosaurs\"], '-x')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to see the total mass in every bin, we add the `CellMassMsun` field with no weight.  Specifying `weight=None` will simply take the total value in every bin and add that up."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prof.add_fields([\"CellMassMsun\"], weight=None)\n",
      "pylab.loglog(prof[\"Density\"], prof[\"CellMassMsun\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also specify accumulation, which sums all the bins, from left to right.  Note that for 2D and 3D profiles, this needs to be a tuple of length 2 or 3."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prof.add_fields([\"CellMassMsun\"], weight=None, accumulation=True)\n",
      "pylab.loglog(prof[\"Density\"], prof[\"CellMassMsun\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Advanced Derived Fields\n",
      "\n",
      "*This section can be skipped!*\n",
      "\n",
      "You can also define fields that require extra zones.  This is useful, for instance, if you want to take the average, or apply a stencil.  yt provides fields like `DivV` that do this internally.  This example is a very busy example of how to do it.  You need to specify the validator `ValidateSpatial` with the number of extra zones *on each side* of the grid that you need, and then inside your function you need to return a field *with those zones stripped off*.  So by necessity, the arrays returned by `data[something]` will have larger spatial extent than what should be returned by the function itself.  If you specify that you need 0 extra zones, this will also work and will simply supply a `grid` object for the field."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@derived_field(name = \"AveragedTemperature\",\n",
      "               validators = [ValidateSpatial(1)],\n",
      "               units = r\"K\")\n",
      "def _AveragedTemperature(field, data):\n",
      "    nx, ny, nz = data[\"Temperature\"].shape\n",
      "    new_field = na.zeros((nx-2,ny-2,nz-2), dtype='float64')\n",
      "    weight_field = na.zeros((nx-2,ny-2,nz-2), dtype='float64')\n",
      "    i_i, j_i, k_i = na.mgrid[0:3,0:3,0:3]\n",
      "    for i,j,k in zip(i_i.ravel(),j_i.ravel(),k_i.ravel()):\n",
      "        sl = [slice(i,nx-(2-i)),slice(j,ny-(2-j)),slice(k,nz-(2-k))]\n",
      "        new_field += data[\"Temperature\"][sl] * data[\"CellMass\"][sl]\n",
      "        weight_field += data[\"CellMass\"][sl]\n",
      "    # Now some fancy footwork\n",
      "    new_field2 = na.zeros((nx,ny,nz))\n",
      "    new_field2[1:-1,1:-1,1:-1] = new_field/weight_field\n",
      "    return new_field2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, once again, we can access `AveragedTemperature` just like any other field.  Note that because it requires ghost zones, this will be a much slower process!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pf = load(\"IsolatedGalaxy/galaxy0030/galaxy0030\")\n",
      "dd = pf.h.all_data()\n",
      "(tmin, tmax), (atmin, atmax) = dd.quantities[\"Extrema\"]([\"Temperature\", \"AveragedTemperature\"])\n",
      "print tmin, tmax, atmin, atmax\n",
      "print tmin / atmin, tmax / atmax"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Field Parameters\n",
      "\n",
      "Field parameters are a method of passing information to derived fields.  For instance, you might pass in information about a vector you want to use as a basis for a coordinate transformation.  yt often uses things like `bulk_velocity` to identify velocities that should be subtracted off.  Here we show how that works:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sp_small = pf.h.sphere(\"max\", (1.0, 'kpc'))\n",
      "bv = sp_small.quantities[\"BulkVelocity\"]()\n",
      "\n",
      "sp = pf.h.sphere(\"max\", (0.1, 'mpc'))\n",
      "rv1 = sp.quantities[\"Extrema\"](\"RadialVelocity\")\n",
      "\n",
      "sp.clear_data()\n",
      "sp.set_field_parameter(\"bulk_velocity\", bv)\n",
      "rv2 = sp.quantities[\"Extrema\"](\"RadialVelocity\")\n",
      "\n",
      "print bv\n",
      "print rv1\n",
      "print rv2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}