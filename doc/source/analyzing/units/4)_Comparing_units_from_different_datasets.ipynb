{
 "metadata": {
  "name": "",
  "signature": "sha256:273a23e3a20b277a9e5ea7117b48cf19013c331d0893e6e9d21896e97f59aceb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Units that refer to the internal simulation coordinate system will have different CGS conversion factors in different datasets.  Depending on how a unit system is implemented, this could add an element of uncertainty when we compare dimensional arrays instances produced by different unit systems.  Fortunately, this is not a problem for `YTArray` since all `YTArray` unit systems are defined in terms of physical CGS units.\n",
      "\n",
      "As an example, let's load up two enzo datasets from different redshifts in the same cosmology simulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A high redshift output from z ~ 8\n",
      "import yt\n",
      "\n",
      "ds1 = yt.load('Enzo_64/DD0002/data0002')\n",
      "print \"z = %s\" % ds1.current_redshift\n",
      "print \"Internal length units = %s\" % ds1.length_unit\n",
      "print \"Internal length units in cgs = %s\" % ds1.length_unit.in_cgs()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A low redshift output from z ~ 0\n",
      "ds2 = yt.load('Enzo_64/DD0043/data0043')\n",
      "print \"z = %s\" % ds2.current_redshift\n",
      "print \"Internal length units = %s\" % ds2.length_unit\n",
      "print \"Internal length units in cgs = %s\" % ds2.length_unit.in_cgs()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given that these are from the same simulation in comoving units, the CGS length units are different by a factor of $(1+z_1)/(1+z_2)$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds2.length_unit.in_cgs()/ds1.length_unit.in_cgs() == (1+ds1.current_redshift)/(1+ds2.current_redshift)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's not necessary to convert to CGS units either.  yt will automatically account for the fact that a comoving megapaersec in the first output is physically different compared to a comoving megaparsec in the second output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds2.length_unit/ds1.length_unit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time series analysis is also straightforward.  Since dimensional arrays and quantities carry around the conversion factors to CGS with them, we can safely pickle them, share them with other processors, or combine them without worrying about differences in unit definitions.\n",
      "\n",
      "The following snippet, which iterates over a time series and saves the `length_unit` quantity to a storage dictionary. This should work correctly on one core or in a script run in parallel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import yt\n",
      "yt.enable_parallelism()\n",
      "\n",
      "ts = yt.load(\"Enzo_64/DD????/data????\")\n",
      "\n",
      "storage = {}\n",
      "\n",
      "for sto, ds in ts.piter(storage=storage):\n",
      "    sto.result_id = ds.current_time\n",
      "    sto.result = ds.length_unit\n",
      "\n",
      "if yt.is_root():\n",
      "    for t in sorted(storage.keys()):\n",
      "        print t.in_units('Gyr'), storage[t].in_units('Mpc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
