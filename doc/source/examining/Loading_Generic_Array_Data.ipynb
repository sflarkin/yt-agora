{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even if your data is not strictly related to fields commonly used in\n",
      "astrophysical codes or your code is not supported yet, you can still feed it to\n",
      "`yt` to use its advanced visualization and analysis facilities. The only\n",
      "requirement is that your data can be represented as three-dimensional NumPy arrays with a consistent grid structure. What follows are some common examples of loading in generic array data that you may find useful. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Generic Unigrid Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simplest case is that of a single grid of data spanning the domain, with one or more fields. The data could be generated from a variety of sources; we'll just give three common examples:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data generated \"on-the-fly\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most common example is that of data that is generated in memory from the currently running script or notebook. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from yt.imods import *\n",
      "from yt.utilities.physical_constants import cm_per_kpc, cm_per_mpc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, we'll just create a 3-D array of random floating-point data using NumPy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr = np.random.random(size=(64,64,64))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To load this data into `yt`, we need to assign it a field name, in this case \"Density\", and place it into a dictionary. Then, we call `load_uniform_grid`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = dict(Density = arr)\n",
      "bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5]])\n",
      "pf = load_uniform_grid(data, arr.shape, cm_per_mpc, bbox=bbox, nprocs=64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`load_uniform_grid` takes the following arguments and optional keywords:\n",
      "\n",
      "* `data` : This is a dict of numpy arrays, where the keys are the field names.\n",
      "* `domain_dimensions` : The domain dimensions of the unigrid\n",
      "* `sim_unit_to_cm` : Conversion factor from simulation units to centimeters\n",
      "* `bbox` : Size of computational domain in units sim_unit_to_cm\n",
      "* `nprocs` : If greater than 1, will create this number of subarrays out of data\n",
      "* `sim_time` : The simulation time in seconds\n",
      "* `periodicity` : A tuple of booleans that determines whether the data will be treated as periodic along each axis\n",
      "\n",
      "This example creates a `yt`-native parameter file `pf` that will treat your array as a\n",
      "density field in cubic domain of 3 Mpc edge size (3 * 3.0856e24 cm) and\n",
      "simultaneously divide the domain into `nprocs` = 64 chunks, so that you can take advantage\n",
      "of the underlying parallelism. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting `pf` functions exactly like a parameter file from any other dataset--it can be sliced, and we can show the grid boundaries:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slc = SlicePlot(pf, 2, [\"Density\"])\n",
      "slc.set_cmap(\"Density\", \"Blues\")\n",
      "slc.annotate_grids(cmap=None)\n",
      "slc.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Particle fields are detected as one-dimensional fields. The number of\n",
      "particles is set by the `number_of_particles` key in\n",
      "`data`. Particle fields are then added as one-dimensional arrays in\n",
      "a similar manner as the three-dimensional grid fields:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posx_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
      "posy_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
      "posz_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
      "data = dict(Density = np.random.random(size=(64,64,64)), \n",
      "            number_of_particles = 10000,\n",
      "            particle_position_x = posx_arr, \n",
      "\t        particle_position_y = posy_arr,\n",
      "\t        particle_position_z = posz_arr)\n",
      "bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5]])\n",
      "pf = load_uniform_grid(data, data[\"Density\"].shape, cm_per_mpc, bbox=bbox, nprocs=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example only the particle position fields have been assigned. `number_of_particles` must be the same size as the particle\n",
      "arrays. If no particle arrays are supplied then `number_of_particles` is assumed to be zero. Take a slice, and overlay particle positions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slc = SlicePlot(pf, \"z\", [\"Density\"])\n",
      "slc.set_cmap(\"Density\", \"Blues\")\n",
      "slc.annotate_particles(0.25, p_size=12.0, col=\"Red\")\n",
      "slc.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "HDF5 data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "HDF5 is a convenient format to store data. If you have unigrid data stored in an HDF5 file, it is possible to load it into memory and then use `load_uniform_grid` to get it into `yt`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import h5py\n",
      "from yt.config import ytcfg\n",
      "data_dir = ytcfg.get('yt','test_data_dir')\n",
      "f = h5py.File(data_dir+\"/UnigridData/turb_vels.h5\", \"r\") # Read-only access to the file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The HDF5 file handle's keys correspond to the datasets stored in the file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print f.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can iterate over the items in the file handle to get the data into a dictionary, which we will then load:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {k:v for k,v in f.items()}\n",
      "bbox = np.array([[-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5]])\n",
      "pf = load_uniform_grid(data, data[\"Density\"].shape, 250.*cm_per_kpc, bbox=bbox, nprocs=8, periodicity=(False,False,False))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the data came from a simulation which was 250 kpc on a side. An example projection of two fields:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prj = ProjectionPlot(pf, \"z\", [\"z-velocity\",\"Temperature\"], weight_field=\"Density\")\n",
      "prj.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Volume Rendering Loaded Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volume rendering requires defining a `TransferFunction` to map data to color and opacity and a `camera` to create a viewport and render the image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Find the min and max of the field\n",
      "mi, ma = pf.h.all_data().quantities[\"Extrema\"]('temperature')[0]\n",
      "#Reduce the dynamic range\n",
      "mi += 1.5e7\n",
      "ma -= 0.81e7"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a Transfer Function that goes from the minimum to the maximum of the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf = ColorTransferFunction((mi, ma), grey_opacity=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define the properties and size of the `camera` viewport:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Choose a vector representing the viewing direction.\n",
      "L = [0.5, 0.5, 0.5]\n",
      "# Define the center of the camera to be the domain center\n",
      "c = pf.domain_center\n",
      "# Define the width of the image\n",
      "W = 1.5*pf.domain_width\n",
      "# Define the number of pixels to render\n",
      "Npixels = 512 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a `camera` object and "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cam = pf.h.camera(c, L, W, Npixels, tf, fields=['temperature'],\n",
      "                  north_vector=[0,0,1], steady_north=True, \n",
      "                  sub_samples=5, no_ghost=False,log_fields=False)\n",
      "\n",
      "cam.transfer_function.map_to_colormap(mi,ma, \n",
      "                                      scale=15.0, colormap='algae')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cam.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "FITS image data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The FITS file format is a common astronomical format for 2-D images, but it can store three-dimensional data as well. The [AstroPy](http://www.astropy.org) project has modules for FITS reading and writing, which were incorporated from the [PyFITS](http://www.stsci.edu/institute/software_hardware/pyfits) library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import astropy.io.fits as pyfits\n",
      "# Or, just import pyfits if that's what you have installed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using `pyfits` we can open a FITS file. If we call `info()` on the file handle, we can figure out some information about the file's contents. The file in this example has a primary HDU (header-data-unit) with no data, and three HDUs with 3-D data. In this case, the data consists of three velocity fields:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = pyfits.open(data_dir+\"/UnigridData/velocity_field_20.fits.gz\")\n",
      "f.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put it into a dictionary in the same way as before, but we slice the file handle `f` so that we don't use the `PrimaryHDU`. `hdu.name` is the field name and `hdu.data` is the actual data. We can check that we got the correct fields. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {hdu.name.lower():hdu.data for hdu in f[1:]}\n",
      "print data.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we load the data into `yt`. This particular file doesn't have any coordinate information, but let's assume that the box size is a Mpc. Since these are velocity fields, we can overlay velocity vectors on slices, just as if we had loaded in data from a supported code. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pf = load_uniform_grid(data, data[\"x-velocity\"].shape, cm_per_mpc)\n",
      "slc = SlicePlot(pf, \"x\", [\"x-velocity\",\"y-velocity\",\"z-velocity\"])\n",
      "slc.annotate_velocity()\n",
      "slc.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Generic AMR Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a similar fashion to unigrid data, data gridded into rectangular patches at varying levels of resolution may also be loaded into `yt`. In this case, a list of grid dictionaries should be provided, with the requisite information about each grid's properties. This example sets up two grids: a top-level grid (`level == 0`) covering the entire domain and a subgrid at `level == 1`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_data = [\n",
      "    dict(left_edge = [0.0, 0.0, 0.0],\n",
      "         right_edge = [1.0, 1.0, 1.],\n",
      "         level = 0,\n",
      "         dimensions = [32, 32, 32]), \n",
      "    dict(left_edge = [0.25, 0.25, 0.25],\n",
      "         right_edge = [0.75, 0.75, 0.75],\n",
      "         level = 1,\n",
      "         dimensions = [32, 32, 32])\n",
      "   ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll just fill each grid with random density data, with a scaling with the grid refinement level."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for g in grid_data: g[\"Density\"] = np.random.random(g[\"dimensions\"]) * 2**g[\"level\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Particle fields are supported by adding 1-dimensional arrays to each `grid` and\n",
      "setting the `number_of_particles` key in each `grid`'s dict. If a grid has no particles, set `number_of_particles = 0`, but the particle fields still have to be defined since they are defined elsewhere; set them to empty NumPy arrays:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_data[0][\"number_of_particles\"] = 0 # Set no particles in the top-level grid\n",
      "grid_data[0][\"particle_position_x\"] = np.array([]) # No particles, so set empty arrays\n",
      "grid_data[0][\"particle_position_y\"] = np.array([]) \n",
      "grid_data[0][\"particle_position_z\"] = np.array([])\n",
      "grid_data[1][\"number_of_particles\"] = 1000\n",
      "grid_data[1][\"particle_position_x\"] = np.random.uniform(low=0.25, high=0.75, size=1000)\n",
      "grid_data[1][\"particle_position_y\"] = np.random.uniform(low=0.25, high=0.75, size=1000)\n",
      "grid_data[1][\"particle_position_z\"] = np.random.uniform(low=0.25, high=0.75, size=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, call `load_amr_grids`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pf = load_amr_grids(grid_data, [32, 32, 32], 1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`load_amr_grids` also takes the same keywords `bbox` and `sim_time` as `load_uniform_grid`. Let's take a slice:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slc = SlicePlot(pf, \"z\", [\"Density\"])\n",
      "slc.annotate_particles(0.25, p_size=15.0, col=\"Pink\")\n",
      "slc.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Caveats for Loading Generic Array Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Units will be incorrect unless the data has already been converted to cgs.\n",
      "* Particles may be difficult to integrate.\n",
      "* Data must already reside in memory before loading it in to `yt`, whether it is generated at runtime or loaded from disk. \n",
      "* Some functions may behave oddly, and parallelism will be disappointing or non-existent in most cases.\n",
      "* No consistency checks are performed on the index\n",
      "* Consistency between particle positions and grids is not checked; `load_amr_grids` assumes that particle positions associated with one grid are not bounded within another grid at a higher level, so this must be ensured by the user prior to loading the grid data. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}