.. _loading-data:

Loading Data
============

This section contains information on how to load data into ``yt``, as well as
some important caveats about different data formats.

.. _loading-enzo-data:

Enzo Data
---------

Enzo data is fully supported and cared for by Matthew Turk.  To load an Enzo
dataset, you can use the ``load`` command provided by ``yt.mods`` and supply to
it the parameter file name.  This would be the name of the output file, and it
contains no extension.  For instance, if you have the following files:

.. code-block:: none

   DD0010/
   DD0010/data0010
   DD0010/data0010.hierarchy
   DD0010/data0010.cpu0000
   DD0010/data0010.cpu0001
   DD0010/data0010.cpu0002
   DD0010/data0010.cpu0003

You would feed the ``load`` command the filename ``DD0010/data0010`` as
mentioned.

.. code-block:: python

   from yt.mods import *
   pf = load("DD0010/data0010")

.. rubric:: Caveats

* There are no major caveats for Enzo usage
* Units should be correct, if you utilize standard unit-setting routines.  yt
  will notify you if it cannot determine the units, although this
  notification will be passive.
* 2D and 1D data are supported, but the extraneous dimensions are set to be
  of length 1.0

.. _loading-orion-data:

Boxlib Data
-----------

yt has been tested with Boxlib data generated by Orion, Nyx, Maestro and
Castro.  Currently it is cared for by a combination of Andrew Myers, Chris
Malone, and Matthew Turk.

To load a Boxlib dataset, you can use the ``load`` command provided by
``yt.mods`` and supply to it the directory file name.  **You must also have the
``inputs`` file in the base directory.**  For instance, if you were in a
directory with the following files:

.. code-block:: none

   inputs
   pltgmlcs5600/
   pltgmlcs5600/Header
   pltgmlcs5600/Level_0
   pltgmlcs5600/Level_0/Cell_H
   pltgmlcs5600/Level_1
   pltgmlcs5600/Level_1/Cell_H
   pltgmlcs5600/Level_2
   pltgmlcs5600/Level_2/Cell_H
   pltgmlcs5600/Level_3
   pltgmlcs5600/Level_3/Cell_H
   pltgmlcs5600/Level_4
   pltgmlcs5600/Level_4/Cell_H

You would feed it the filename ``pltgmlcs5600``:

.. code-block:: python

   from yt.mods import *
   pf = load("pltgmlcs5600")

.. rubric:: Caveats

* There are no major caveats for Orion usage

.. _loading-flash-data:

FLASH Data
----------

FLASH HDF5 data is *mostly* supported and cared for by John ZuHone.  To load a
FLASH dataset, you can use the ``load`` command provided by ``yt.mods`` and
supply to it the file name of a plot file or checkpoint file, but particle
files are not currently directly loadable by themselves, due to the
fact that they typically lack grid information. For instance, if you were in a directory with
the following files:

.. code-block:: none

   cosmoSim_coolhdf5_chk_0026

You would feed it the filename ``cosmoSim_coolhdf5_chk_0026``:

.. code-block:: python

   from yt.mods import *
   pf = load("cosmoSim_coolhdf5_chk_0026")

If you have a FLASH particle file that was created at the same time as
a plotfile or checkpoint file (therefore having particle data
consistent with the grid structure of the latter), its data may be loaded with the
``particle_filename`` optional argument:

.. code-block:: python

    from yt.mods import *
    pf = load("radio_halo_1kpc_hdf5_plt_cnt_0100", particle_filename="radio_halo_1kpc_hdf5_part_0100")

.. rubric:: Caveats

* Please be careful that the units are correctly utilized; yt assumes cgs
* Velocities and length units will be scaled to comoving coordinates if yt is
  able to discern you are examining a cosmology simulation; particle and grid
  positions will not be.
* Domains may be visualized assuming periodicity.

.. _loading-ramses-data:

RAMSES Data
-----------

RAMSES data enjoys preliminary support and is cared for by Matthew Turk.  If
you are interested in taking a development or stewardship role, please contact
him.  To load a RAMSES dataset, you can use the ``load`` command provided by
``yt.mods`` and supply to it the ``info*.txt`` filename.  For instance, if you
were in a directory with the following files:

.. code-block:: none

   output_00007
   output_00007/amr_00007.out00001
   output_00007/grav_00007.out00001
   output_00007/hydro_00007.out00001
   output_00007/info_00007.txt
   output_00007/part_00007.out00001

You would feed it the filename ``output_00007/info_00007.txt``:

.. code-block:: python

   from yt.mods import *
   pf = load("output_00007/info_00007.txt")

.. rubric:: Caveats

* Please be careful that the units are correctly set!  This may not be the
  case for RAMSES data
* Upon instantiation of the hierarchy, yt will attempt to regrid the entire
  domain to ensure minimum-coverage from a set of grid patches.  (This is
  described in the yt method paper.)  This is a time-consuming process and it
  has not yet been written to be stored between calls.
* Particles are not supported
* Parallelism will not be terribly efficient for large datasets
* There may be occasional segfaults on multi-domain data, which do not
  reflect errors in the calculation

If you are interested in helping with RAMSES support, we are eager to hear from
you!

.. _loading-art-data:

ART Data
--------

ART data enjoys preliminary support and is supported by Christopher Moody.
Please contact the ``yt-dev`` mailing list if you are interested in using yt
for ART data, or if you are interested in assisting with development of yt to
work with ART data.

At the moment, the ART octree is 'regridded' at each level to make the native
octree look more like a mesh-based code. As a result, the initial outlay
is about ~60 seconds to grid octs onto a mesh. This will be improved in 
``yt-3.0``, where octs will be supported natively. 

To load an ART dataset you can use the ``load`` command provided by 
``yt.mods`` and passing the gas mesh file. It will search for and attempt 
to find the complementary dark matter and stellar particle header and data 
files. However, your simulations may not follow the same naming convention.

So for example, a single snapshot might have a series of files looking like
this:

.. code-block:: none

   10MpcBox_csf512_a0.300.d    #Gas mesh
   PMcrda0.300.DAT             #Particle header
   PMcrs0a0.300.DAT            #Particle data (positions,velocities)
   stars_a0.300.dat            #Stellar data (metallicities, ages, etc.)

The ART frontend tries to find the associated files matching the above, but
if that fails you can specify ``file_particle_data``,``file_particle_data``,
``file_star_data`` in addition to the specifying the gas mesh. You also have 
the option of gridding particles, and assigning them onto the meshes.
This process is in beta, and for the time being it's probably  best to leave
``do_grid_particles=False`` as the default.

To speed up the loading of an ART file, you have a few options. You can turn 
off the particles entirely by setting ``discover_particles=False``. You can
also only grid octs up to a certain level, ``limit_level=5``, which is useful
when debugging by artificially creating a 'smaller' dataset to work with.

Finally, when stellar ages are computed we 'spread' the ages evenly within a
smoothing window. By default this is turned on and set to 10Myr. To turn this 
off you can set ``spread=False``, and you can tweak the age smoothing window
by specifying the window in seconds, ``spread=1.0e7*265*24*3600``. 

.. code-block:: python
    
   from yt.mods import *

   file = "/u/cmoody3/data/art_snapshots/SFG1/10MpcBox_csf512_a0.460.d"
   pf = load(file,discover_particles=True,grid_particles=2,limit_level=3)
   pf.h.print_stats()
   dd=pf.h.all_data()
   print np.sum(dd['particle_type']==0)

In the above example code, the first line imports the standard yt functions,
followed by defining the gas mesh file. It's loaded only through level 3, but
grids particles on to meshes on level 2 and higher. Finally, we create a data
container and ask it to gather the particle_type array. In this case ``type==0``
is for the most highly-refined dark matter particle, and we print out how many
high-resolution star particles we find in the simulation.  Typically, however,
you shouldn't have to specify any keyword arguments to load in a dataset.

.. _loading-numpy-array:

Generic Array Data
------------------

Even if your data is not strictly related to fields commonly used in
astrophysical codes or your code is not supported yet, you can still feed it to
``yt`` to use its advanced visualization and analysis facilities. The only
requirement is that your data can be represented as one or more uniform, three
dimensional numpy arrays. Assuming that you have your data in ``arr``,
the following code:

.. code-block:: python

   from yt.frontends.stream.api import load_uniform_grid

   data = dict(Density = arr)
   bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [1.5, 1.5]])
   pf = load_uniform_grid(data, arr.shape, 3.08e24, bbox=bbox, nprocs=12)

will create ``yt``-native parameter file ``pf`` that will treat your array as
density field in cubic domain of 3 Mpc edge size (3 * 3.08e24 cm) and
simultaneously divide the domain into 12 chunks, so that you can take advantage
of the underlying parallelism. 

Particle fields are detected as one-dimensional fields. The number of
particles is set by the ``number_of_particles`` key in
``data``. Particle fields are then added as one-dimensional arrays in
a similar manner as the three-dimensional grid fields:

.. code-block:: python

   from yt.frontends.stream.api import load_uniform_grid

   data = dict(Density = dens, 
               number_of_particles = 1000000,
               particle_position_x = posx_arr, 
	       particle_position_y = posy_arr,
	       particle_position_z = posz_arr)
   bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [1.5, 1.5]])
   pf = load_uniform_grid(data, arr.shape, 3.08e24, bbox=bbox, nprocs=12)

where in this exampe the particle position fields have been assigned. ``number_of_particles`` must be the same size as the particle
arrays. If no particle arrays are supplied then ``number_of_particles`` is assumed to be zero. 

.. rubric:: Caveats

* Units will be incorrect unless the data has already been converted to cgs.
* Particles may be difficult to integrate.
* Data must already reside in memory.

.. _loading-amr-data:

Generic AMR Data
----------------

It is possible to create native ``yt`` parameter file from Python's dictionary
that describes set of rectangular patches of data of possibly varying
resolution. 

.. code-block:: python

   from yt.frontends.stream.api import load_amr_grids

   grid_data = [
       dict(left_edge = [0.0, 0.0, 0.0],
            right_edge = [1.0, 1.0, 1.],
            level = 0,
            dimensions = [32, 32, 32],
            number_of_particles = 0)
       dict(left_edge = [0.25, 0.25, 0.25],
            right_edge = [0.75, 0.75, 0.75],
            level = 1,
            dimensions = [32, 32, 32],
            number_of_particles = 0)
   ]
  
   for g in grid_data:
       g["Density"] = np.random.random(g["dimensions"]) * 2**g["level"]
  
   pf = load_amr_grids(grid_data, [32, 32, 32], 1.0)

Particle fields are supported by adding 1-dimensional arrays and
setting the ``number_of_particles`` key to each ``grid``'s dict:

.. code-block:: python

    for g in grid_data:
        g["number_of_particles"] = 100000
        g["particle_position_x"] = np.random.random((g["number_of_particles"]))

.. rubric:: Caveats

* Units will be incorrect unless the data has already been converted to cgs.
* Some functions may behave oddly, and parallelism will be disappointing or
  non-existent in most cases.
* No consistency checks are performed on the hierarchy
* Data must already reside in memory.
* Consistency between particle positions and grids is not checked;
  ``load_amr_grids`` assumes that particle positions associated with one grid are
  not bounded within another grid at a higher level, so this must be
  ensured by the user prior to loading the grid data. 

